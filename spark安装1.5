1.ubuntu系统开启root密码，能远程登陆root用户，
并且root用户能远程ssh ---免密码---  登陆
主节点，root用户能ssh localhost（就是主节点也要ssh-copy-i binend   主节点域名）

2.配置环境变了
sudo gedit /etc/profile
export PATH=$PATH:/home/titanic/soft/spark1.5/spark-1.5.0-bin-hadoop2.6/bin

3.要把hive的配置/opt/cloudera/parcels/CDH/lib/hive/conf/hive-site.xml,文件
这个文件拷贝到spark/conf/目录下面

4.配置spark  目录下面conf/spark-env.sh
----------------------------------------------------------------------------
#jdk环境变了
export JAVA_HOME=/usr/lib/jvm/java-7-oracle-cloudera

#hive的jar目录
export CLASSPATH=$CLASSPATH:/opt/cloudera/parcels/CDH/lib/hive/lib
#export SCALA_HOME=/opt/cloudera/parcels/CDH/lib/scala

#主节点的域名host
export SPARK_MASTER_IP=binend

#端口，默认
export SPARK_MASTER_PORT=7077

#spark使用内存1G
export SPARK_WORKER_MEMORY=1g

#spark使用on-yarn模式，要配置hadoop的onf目录，这里配置的是cloudera的hadoop的conf目录
export HADOOP_CONF_DIR=/opt/cloudera/parcels/CDH/lib/hadoop/etc/hadoop
export YARN_CONF_DIR=/opt/cloudera/parcels/CDH/lib/hadoop-yarn/etc/hadoop


#***********************************************
#这个是spark链接hadoop的yarn模式下的默认配置
#export SPARK_EXECUTOR_INSTANCES=6
#export SPARK_EXECUTOR_CORES=2
#export SPARK_EXECUTOR_MEMORY=6G
#export SPARK_DRIVER_MEMORY=2G
#在yarn中默认显示的名称
export SPARK_YARN_APP_NAME=tCloudSpark1.5.1
#***********************************************

#hive的配置目录使用spark，运行hive的sql
export HIVE_CONF_DIR=/opt/cloudera/parcels/CDH/lib/hive/conf
#export HIVE_SERVER2_THRIFT_PORT=10001
#export HIVE_SERVER2_THRIFT_BIND_HOST=binend

#照抄
export SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cloudera/parcels/CDH/lib/hive/lib/mysql-connector-java-5.1.36.jar
export SPARK_JAR=/opt/cloudera/parcels/CDH/lib/spark-1.5.0/lib/spark-assembly-1.5.0-hadoop2.6.0.jar
-----------------------------------------------------------------------------------
4.配置spark目录下conf/slaves 文件  子节点host
binend
titanic
lan

5.启动集群。启动主节点sbin目录下面start-all.sh
6.关闭集群。关闭主节点sbin目录下面stop-all.sh  很慢，要等几分钟
--------------------------------------------------------------
spark启动./sbin/start-thriftserver.sh 模式,说明,就是启动一个jdbc服务.连接spark集群,spark集群使用yarn模式.
就相当于程序代码通过spark访问hive里面数据库的表.可以通过jdbc连接
./sbin/start-thriftserver.sh --master yarn --driver-memory 2G --executor-memory 6G  --hiveconf hive.server2.thrift.port=10001 --name spark-thriftserver

jdbc URL
jdbc:hive2://t1m1.tcloud.com:10001/data2
