
1.关闭防火墙
sudo ufw disable

2.修改hostname

3.ssh免密码登录
ssh-keygen -t rsa

每台机器都运行
ssh-copy-id binend@binend01
ssh-copy-id binend@binend02
ssh-copy-id binend@binend03
ssh-copy-id binend@binend04
ssh-copy-id binend@binend05

4.安装jdk

5.下载hadoop3.1.2

6.在hadoop3.1.2/新建3个文件夹
mkdir -pv /home/binend/soft/hadoop3.1.2/{nn,dn,tmp}

7.修改环境变量，添加hadoop环境比那里
sudo vi /etc/profile

#Add HADOOP_HOME PATH
HADOOP_HOME=/home/binend/soft/hadoop-3.1.2
PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

sources /etc/profile

8.修改配置文件
修改 /hadoop-3.1.2/etc/hadoop/hadoop-env.sh 添加

export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_221
#root是启动hadoop的用户需要修改。默认不用root
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
----------------------------------------------------------------------
core-site.xml 

<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://binend01:8020</value>
        <description>HDFS</description>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/binend/soft/hadoop-3.1.2/hdfs/tmp</value>
        <description>tmp</description>
    </property>
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>
</configuration>
----------------------------------------------------------------------
hdfs-site.xml 

<configuration>
        <property>
                <name>dfs.name.dir</name>
                <value>/home/binend/soft/hadoop-3.1.2/hdfs/nn</value>
        </property>
        <property>
                <name>dfs.data.dir</name>
                <value>/home/binend/soft/hadoop-3.1.2/hdfs/dn</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
</configuration>
----------------------------------------------------------------------
mapred-site.xml

<configuration>
  <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapred.child.java.opts</name>
        <value>-Xmx2048m</value>
    </property>

    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>binend01:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>binend01:19888</value>
    </property>
</configuration>
----------------------------------------------------------------------
yarn-site.xml

<configuration>
<property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>${yarn.resourcemanager.hostname}:8088</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2000</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2000</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>binend01:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>binend01:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>binend01:8031</value>
    </property>
</configuration>

workers文件中添加子节点
binend02
binend03
binend04
binend05

----------------------------------------------------------------------
9.scp分发到各节点相应目录

10.格式化 hadoop namenode -format

11.修改启动名称避免与spark重合
hadoop3.1.2/sbin/start-all.sh -> hadoop-start-all.sh
hadoop3.1.2/sbin/stop-all.sh -> hadoop-stop-all.sh
